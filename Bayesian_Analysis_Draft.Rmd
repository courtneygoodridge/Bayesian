---
title: "Bayesian analysis - draft write up"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This markdown document is my first attempt at writing up a Bayesian analysis of my data. The data I am using is from experiment 1. For this example, I will be investigating how heading angle affects RTs. For a reminder, both Threshold and Accumulator models propose reduced RTs as heading increases.

Firstly I analyse my data using a Bayesian ANOVA from the BayesFactor package. I then go on to use the rstanarm package to fit Bayesian generalised linear models to my data.

## Load packages 

```{r load packages, message =FALSE}
library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(ggplot2) # data vis
library(BayesFactor) # performs Bayesian ANOVA
library(rstanarm) # bayesian regression modelling
library(bayestestR) # describes Bayesian models and posterior distributions
library(bayesplot) # allows plots for posterior predictive distributions
library(loo) # Bayesian model comparison
library(arm) # computes Bayes factor for glm model
library(insight) # get posterior parameters 
library(fitdistrplus) # investigating the distribution of my data
```

## Load data

```{r load my data}
# rm(list = ls()) - clear global environment
# home working directory
# setwd("C:/Users/Courtney/Documents/PhD/Project/Experiment_Code/experiment_1")
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/experiment_1")
temp = list.files(pattern = c("magnitudedata", "*.csv"))
myfiles = lapply(temp, read.csv)
magnitudedata <- do.call(rbind.data.frame, myfiles)
```

## Computing Bayesian ANOVA using BayesFactor package

```{r computing Bayesian ANOVA using BayesFactor}

magnitudedata <- magnitudedata %>%
  dplyr::filter(heading > 0)

magnitudedata$heading <- as.factor(magnitudedata$heading)

bf <- anovaBF(FirstSteeringTime ~ heading, data = magnitudedata)
bf

```

Using the anovaBF function tests the full hypothesis model versus the null model hypothesis. In my example, this will be the main effect of heading. 

The computed Bayes factor is 2.24 x 10^75. This indicates that the current data is 2.24 x 10^75 times more likely under the alternative hypothesis versus the null hypothesis i.e. there is a main effect difference between the levels of my heading factor.

However, am I testing the "right" hypothesis? Richard Morey (https://richarddmorey.org/2015/01/multiple-comparisons-with-bayesfactor-part-2-order-restrictions/#comment-7782) proposed that testing for differences between your factor levels refers to an *unconstrained model* (unconstrained in the sense that I am looking for any differences between my factor levels without specification). I actually have specific predictions on how my factor levels will differ. This refers to an *order restriction model*:

0.5 > 1.0 > 1.5 > 2.0

Already I have Bayes factor for the full model versus the null model. My order restriction model will allow me to make 2 more comparisons with my Bayes factor:

- Full model versus the order restriction model
- Order contraint model versus null model

Bayes factor is the degree of change from prior to posterior odds. Hence if we can compute prior odds, we can compute the Bayes factor. Because I have not specified a prior distribution, my prior odds have equal probability across my factor levels. With 4 factors, the prior odds are 1/8 as there are 8 orderings.

## Sampling and plotting posterior distribution

```{r sample from the posterior distribution}
samples <-  posterior(bf, iterations = 10000)
head(samples)

summary(samples[,2:5])

plot(samples[,2:3])
plot(samples[,4:5])
```

The *posterior* function allows me to sample from the posterior distribution of each heading.

I think the *mu* parameter refers to an estimated population mean parameter for reaction time. Each heading level is then compared to this mu parameter to compute the effect.

The plots show the posterior distributions for each heading level and the standardised effect size. For the *0.5 heading*, the HDI spans across positive effect sizes (i.e. reactions times are higher than *mu*). For *1.0 heading* the HDI spans across zero (reaction times are the same as *mu*). For *1.5 and 2.0 heading* the HDIs span negative standardised effects sizes (reaction times are lower than *mu*).

However these HDIs do how my order restrictions...

## Creating order restriction model

```{r order restriction  model}
## check order restriction 
consistent = (samples[, "heading-1.5"] > samples[, "heading-2"]) &
  (samples[, "heading-1"] > samples[, "heading-1.5"]) &
  (samples[, "heading-0.5"] > samples[, "heading-1"])

N_consistent = sum(consistent)
```

I construct order restrictions whereby I expect larger RTs for 1.5 than for 2, 1 than for 1.5, and 0.5 than for 1.

## Order restriction  model versus full model

```{r restriction versus full model}
bf_contraint_against_full = (N_consistent / 10000) / (1 / 8)
bf_contraint_against_full
```

The posterior probability is the number of consistent order restrictions from the posterior (*n_consistent*) divided by the number of samples (10000). Dividing the posterior by the prior distribution probabilites to get the Bayes factor of the restriction model versus the full model.

We generate a Bayes factor of 8. Hence the data has changed my opinion of the data, in favour of the restriction model, by a factor of 8. 

## Order restriction model versus null model

```{r restriction versus the null model}
## Convert bf1 to a number so that we can multiply it
bf_full_against_null = as.vector(bf)

## Use transitivity to compute desired Bayes factor
bf_restriction_against_null <- bf_contraint_against_full * bf_full_against_null
bf_restriction_against_null

```

If I multiple the Bayes factor from the full model by the Bayes factor of the contraint versus the full model, I can generate the Bayes factor for the restriction model versus the null model. This generates a Bayes factor of 1.79 x 10^76. This indicates the data is 1.79 x 10^76 more likely under the restriction model than under the null.

Overall, I can say that there is more evidence for restriction model versus the null than for the standard main effect model versus the null. However with a Bayes factor of 8, I would remain inconclusive on the restriction versus full model comparison.

## Conclusions so far

Using the anovaBF function gives me a nice introduction to Bayesian analyses. However even a Bayesian ANOVA relies on my error being normally distributed (which it is not). Because of this, I investigated Bayesian generalised linear models where I could account for my normal distribution of errors.

## Looking at the distribution of my data

```{r how is my data distributed}
fit_g  <- fitdist(magnitudedata$FirstSteeringTime, "gamma")

fit_ln <- fitdist(magnitudedata$FirstSteeringTime, "lnorm")

par(mfrow = c(2,2))
plot.legend <- c("gamma", "lognormal")
denscomp(list(fit_g, fit_ln), legendtext = plot.legend)
cdfcomp (list(fit_g, fit_ln), legendtext = plot.legend)
qqcomp  (list(fit_g, fit_ln), legendtext = plot.legend)
ppcomp  (list(fit_g, fit_ln), legendtext = plot.legend)
```

Here I looking at the distribution of my data.

*Log-normal distribution* is like a normal distribution but only contains positive values. This creates a skewed distribution with a right tail. These characteristics match my reaction time data, hence the good fit.

*Gamma distribution* is similar to *log-normal distribution* in that it is right skewed, and for continous data. 

For both of these distributions, the shape parameter is held constant whilst the scale parameter shifts. This is good for my data as I have a lot of variance in my responses. As both of these distributions are so simialr, I proceed with *Gamma*.

## Setting my contrasts

```{r construct contrast matrix}
# default contrasts - orthogonal
contrasts(magnitudedata$heading)

# setting my own contrasts - better addresses my research question
contrastmatrix <- cbind(c(-1, 1, 0, 0), c(0, -1, 1, 0), c(0, 0, -1, 1))

contrasts(magnitudedata$heading) <- contrastmatrix

contrasts(magnitudedata$heading)
```

Before I fit my Bayesian generalised linear model, I need to set my contrasts.

The default contrasts assume that the 0.5 heading condiiton is my control. They compare each of my other headings against this. These are my *"orthogonal"* contrasts in that they are linearly and statistically independent (each vector cannot be defined via a linear combination of the other vectors). 

However I did not feel that these were the best contrasts for my research question. I create a contrast matrix with 3 contrasts:

- heading1 <- 1.0 vs 0.5
- heading2 <- 1.5 vs 1.0
- heading3 <- 2.0 vs 1.5

These contrasts allow me to investigate whether RTs decrease as heading values increase. 

**Problem**: I am unsure how to implement a contrast matrix that would match the restriction  model I implemented for the Bayesian ANOVA. Thus this is the closest I can get so far (however I'm sure it is possible).

## Fitting Bayesian regression model

```{r fitting Bayesian GLM model, message = FALSE, warning = FALSE}
fit_glm <- stan_glm(FirstSteeringTime ~ heading, family = Gamma(link = "identity"), adapt_delta = 0.999, data = magnitudedata)
```

The stan_glm function requires a prior. I omitted the option of a prior and defaulted to the weakly informative ones. This is because as the documentation indicates that for many applications, they will work well (https://www.rdocumentation.org/packages/rstanarm/versions/2.19.2/topics/priors)

I set adapt_delta arguement to *0.999* to reduce the stepsize during the model simulation. Stepsize specifies the resolution at which you are simulating through the posterior distribution. If the steps are too large, I might miss something and thus estimates become biased. This is known as divergence. The documentation proposes an adapt_delta arguement value such as this to reduce the chances of divergence (http://mc-stan.org/rstanarm/articles/aov.html)

## Summary of my Bayesian GLM model

```{r summarising the model}
summary(fit_glm)
mean(magnitudedata$FirstSteeringTime)
```
My Rhat values suggest the model succesfully converged.

*mean_ppd* in the estimates table can be used as a heuristic for how well the model describes the data. This value represents a best guess of the *mean posterior predicitive distribution* (distribution of simiulated data based on current data). Ideally this should be equal to the mean of my outcome variable (RT). In this case, the values are similar.

## ROPE intervals 

```{r ROPE intervals and drawing from posterior distributions}
posteriors <- insight::get_parameters(fit_glm)

ggplot(data = posteriors) +
  geom_vline(xintercept = median(posteriors$heading1), color = "red") +
  geom_vline(xintercept = median(posteriors$heading2), color = "orange") +
  geom_vline(xintercept = median(posteriors$heading3), color = "blue") +
  geom_density(aes(x = heading1), fill = "red", alpha = 0.3) +
  geom_density(aes(x = heading2), fill = "orange", alpha = 0.3) +
  geom_density(aes(x = heading3), fill = "blue", alpha = 0.3) +
  geom_vline(xintercept = -.04, color = "black", size = 2) +
  geom_vline(xintercept = 0, color = "black", size = 2) +
  xlim(-.15, .15) +
  xlab("standardised coefficient effect")

# establishing the percentage of the HDIs within the ROPE interval
percentage_in_rope_glm <- rope(fit_glm, ci = 1, range = c(-0.04, 0))

percentage_in_rope_glm

# median from my HDIs of standardised effect size for each contrast

median(posteriors$heading1)
median(posteriors$heading2)
median(posteriors$heading3)
```
The following is generated from the example given here (https://easystats.github.io/bayestestR/articles/example1.html)

The Bayesian GLM demonstrates that for each contrast, heading level is having an negative on the outcome variable i.e. an increase in heading is generating a decrease in RT. However, the Bayesian method does not rely on statistical sigificance in order to understand whether an effect is interesting - instead, the posterior distribution generates a probabilistic view of certain parameters and their uncertainty i.e. the coefficient for certain contrasts. Rather than conclude that there is simply a difference from a point null (0) one method used in Bayesian analyses is to construct a ROPE. 

ROPE stands for "region of practical equivalence". Comparing your alternative hypothesis to a single point null does not make much sense theoretically, as it is very likely there will be a difference (even if it is extremely small). I can define a ROPE which is an interval around a point null that is "practically equivalent" to it.

Kruschke (2018) proposes that for linear models, ROPE intervals are defined as 0.1 (half of a small effect size) multiplied by the standard deviation of the outcome variable. Ordinarily this would generate a ROPE interval of [-0.02, 0.02]. 

However this refers to the a 2 tailed hypothesis. I can be more specific. I know the direction of my effect (negative) thus I double the negative part of the interval and implement a one tailed ROPE interval [-0.04, 0]. These are defined as the **black** lines.

I can then draw from the posterior distributions, plot the HDIs for each of my contrasts and indicate the median most credible value with a vertical line. This provides a nice literal interpretation 

"*95% of the the most credible values for standardised coefficient effect of my contrasts are not practically equivalent to the null*". 

**Sidenote** I get a warning of multicollinearity between 2 of my contrasts (heading1 and heading2). This likely due to my contrast matrix being linearly dependent. 

## Probability of Direction

```{r pd}

p_direction(posteriors)

```

The *Probability of Direction* is a value that ranges from 50% to 100%. It is interpretted as the probability that a parameter is positive or negative. Mathematically it is defined as the percentage of the posterior distribition that is the same sign as the median. The *Probability of Direction* is indepedent of the model and computed from posterior draws. 

It does not detail the magnitude or importance of an effect - rather the probability that an effect is positive and negative. In this respect, it is directly relatable to the frequentist p value. 

For more information see:

(https://easystats.github.io/bayestestR/articles/probability_of_direction.html#methods-comparison)
(https://easystats.github.io/bayestestR/reference/p_direction.html#arguments)

## Posterior predictive distributions

```{r posterior predictive distribution}
# observedresponses
y <- magnitudedata$FirstSteeringTime

# sample draws from the posterior distribution
yrep_glm <- posterior_predict(fit_glm, draws = 50)

color_scheme_set("brightblue")
ppc_dens_overlay(y, yrep_glm[1:50, ])
```

*Posterior distributions* explain unknown parameters (standardised effect size) and provide the 95% most credible values for that parameter. *Posterior predictive distributions* refer to a distribution of future predicted data based upon the data already seen. 

If the model is a good fit, you should be able to simulate similar data based on data generated during the experiment.

My model demonstrates a reasonably good fit and predicts the peak location well, however it under predicts the density of the peak. 

## Computing a Bayes factor

```{r Bayes factor, message = FALSE, warning = FALSE}

bf_glm <- bayesfactor_parameters(fit_glm, null = c(-.04, 0))
bf_glm

```
I can compute my Bayes factor for each contrast. This also allows me to compute my Bayes factor for the alternative hypothesis versus the ROPE interval null hypothesis, rather than a point null. This demonstrates substantial evidence for the alternative hypotheses for each contrast.

Ideally, I would also implement a contrast where I am comparing: *0.5 > 1.0 > 1.5 > 2.0* but as explained above, I'm unsure how to implement that within the contrast matrix (**see Bayesian_restriction markdown for how to do this**. 

This is based upon the example given here (https://easystats.github.io/bayestestR/articles/bayes_factors.html#testing-models-parameters-with-bayes-factors)

## Bayesian GLM model with lognormal family and model comparison

```{r model comparison using the loo package, message = FALSE, warning = FALSE}
fit_glm_log <- stan_glm(FirstSteeringTime ~ heading, family = gaussian(link = "log"), adapt_delta = 0.999, data = magnitudedata)

loo_glm <- loo(fit_glm)

loo_log <- loo(fit_glm_log)

print(loo_compare(loo_log, loo_glm), digits = 3)
```
The *loo_compare* function compares the two Bayesian model fits using the "leave one out" cross validaion method. This method investigates the differences in predictive errors and standard errors for each model. This is encapsulated by the *elpd_diff* (difference in predictive errors) and *se_diff* (difference in standard errors) variables.

Columns within the matrix are computed by generating pairwise comparisons between each model and the model with highest ELPD (i.e. highest predictive accuracy). Because of this, the first row will always be zeros because this is the preferred model compared against itself. 

When the difference (*elpd_diff*) is positive, the expected predictive accuracy is higher for the second model. When it is negative, this favours the model in the first row. Because my *elpd_diff* shows a large negative, model2 (fit_glm) is the preferred model. 

