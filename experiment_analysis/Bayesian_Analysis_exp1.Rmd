---
title: "Bayesian analysis - Experiment 1"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This markdown document is my first attempt at writing up a Bayesian analysis of my data. The data I am using is from experiment 1. For this example, I will be investigating how heading angle affects RTs. For a reminder, both Threshold and Accumulator models propose reduced RTs as heading increases.

I am using the *rstanarm* package to fit a Bayesian generalised linear models to my data. I then compute Bayes factor for the full model contrasts, Bayes factors for the restricted model, invetsigate 95% HDIs, use the HDI + ROPE for inference and the probability of direction.

## Load packages 

```{r message = FALSE}
# rm(list = ls()) 
library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(ggplot2) # data vis
library(BayesFactor) # performs Bayesian ANOVA
library(rstanarm) # bayesian regression modelling
library(bayestestR) # describes Bayesian models and posterior distributions
library(bayesplot) # allows plots for posterior predictive distributions
library(loo) # Bayesian model comparison
library(arm) # computes Bayes factor for glm model
library(insight) # get posterior parameters 
library(fitdistrplus) # investigating the distribution of my data
library(emmeans) # for computing estimated marginal means for Bayesian models
library(fitdistrplus) # finding shape distribution
```

## Saving environment

```{r}
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/Bayesian")
save.image(file = 'BayesModelsEnvExp1.RData')

setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/Bayesian")
load('BayesModelsEnvExp1.RData')
```

## Load data

```{r}
# rm(list = ls()) - clear global environment
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/experiment_1/Data cleaning and modelling/Finished_code")
magnitudedata <- read.csv("magnitudedata.csv")

magnitudedata <- magnitudedata %>%
  dplyr::filter(EarlyResponses == FALSE)
```

## theme settings

```{r}
theme_plot <-   theme(axis.title.x = element_text(size = 15), axis.text.x = element_text(size = 12), axis.title.y = element_text(size = 15), axis.text.y = element_text(size = 12), title = element_text(size = 12), legend.title = element_text(size = 15), legend.text = element_text(size = 15), panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))
```


## Fitting Bayesian regression model

```{r message = FALSE, warning = FALSE}
fit.gamma.default.prior <- stan_glmer(FirstSteeringTime ~ heading + (1 + heading | pNum),
                                      family = Gamma(link = "identity"),
                                      data = magnitudedata)

fit.gamma.narrow.cauchy <- stan_glmer(FirstSteeringTime ~ heading + (1 + heading | pNum),
                                              family = Gamma(link = "identity"),
                                              prior = cauchy(location = 0, scale = 0.2),
                                              data = magnitudedata)

fit.gamma.wide.cauchy <- stan_glmer(FirstSteeringTime ~ heading + (1 + heading | pNum),
                                              family = Gamma(link = "identity"),
                                              prior = cauchy(location = 0, scale = 1),
                                              data = magnitudedata)

models <- list(fit.gamma.default.prior, fit.gamma.narrow.cauchy, fit.gamma.wide.cauchy)
```

The stan_glmer function allows me to fit a Bayesian multilevel model. This is necessary for my data structure as if I fit a standard glm, it assumes that every observation is actually a participant. This will inflate my estimate. By incorporating participants as a random effect, I can exploit the repeated measures design and account for variability in my response variable between subjects.

The function also requires a prior. For the first model, I omitted the option of a prior and defaulted to the weakly informative. This is because as the documentation indicates that for many applications, they will work well (https://www.rdocumentation.org/packages/rstanarm/versions/2.19.2/topics/priors). For the second and third model, I have specified a Cauchy distribution. This distribution situates more density over larger potential effect sizes, making it a more conservative prior estimate. I fit models with two different priors - one with a narrow and wide scale to assess the difference in Bayes factor. Rouder et al (2016) proposes that the difference only affects the Bayes factor slightly but it is something that should be considered. 

Another thing that can be adapted is the adapt_delta arguement within the model. Changing to this to a value of *0.999* for example could reduce the stepsize during the model simulation. Stepsize specifies the resolution at which you are simulating through the posterior distribution. If the steps are too large, I might miss something and thus estimates become biased. This is known as divergence. The documentation proposes an adapt_delta arguement value such as this to reduce the chances of divergence (http://mc-stan.org/rstanarm/articles/aov.html)

Rhat values of 1 suggest the model successfully converged.

*mean_ppd* in the estimates table can be used as a heuristic for how well the model describes the data. This value represents a best guess of the *mean posterior predicitive distribution* (distribution of simiulated data based on current data). Ideally this should be equal to the mean of my outcome variable (RT). In this case, the values are similar.

### Investigating prior distributions

```{r}
"plotting individual models"

for (i in c(1:length(models))){
  
  "select and save temp model"
  mod.temp <- models[[i]]
  
  "save prior distributions"
  prior.temp <- update(mod.temp, prior_PD = TRUE)
  
  "convert priors to dataframe for plotting"
  prior.temp <- as.data.frame(prior.temp)
  
  print(ggplot(data = prior.temp) +
          geom_density(aes(heading), fill = "red", alpha = 0.1))
}
  
prior_summary(fit.gamma.default.prior)
prior_summary(fit.gamma.narrow.cauchy)
prior_summary(fit.gamma.wide.cauchy)

"Saving priors as dataframes"
default.prior <- as.data.frame(update(fit.gamma.default.prior, prior_PD = TRUE)) %>%
  dplyr::mutate(prior = "default")
  
narrow.cauchy.prior <- as.data.frame(update(fit.gamma.narrow.cauchy, prior_PD = TRUE)) %>%
  dplyr::mutate(prior = "cauchy_narrow")

wide.cauchy.prior <- as.data.frame(update(fit.gamma.wide.cauchy, prior_PD = TRUE)) %>%
  dplyr::mutate(prior = "cauchy_wide")

priors = rbind(default.prior, narrow.cauchy.prior, wide.cauchy.prior)

"distribution for fill colour"
ggplot(data = priors, mapping = aes(x = heading, fill = prior)) +
         geom_density(alpha = 0.3) +
         scale_fill_manual(name = "Prior", labels = c("Cauchy, scale = 0.2", "Cauchy, scale = 1", "Normal, scale = 2.5"), values = c("yellow", "red", "blue")) +
  ggtitle("Prior distributions for heading") +
  xlim(-15, 15) +
  theme_plot +
  theme(legend.position = c(0.8, 0.5))
```

The prior summary function gives me a summary for each of model priors. The default prior is a normal distribution. As you can see, it is weakly informative and thus situates fairly equal density over a majority of the estimated effect size values. The Cauchy distribution with a scale  of 1 is a bit more informative, and situates a bit more density over values closer to one. Finally, the Cauchy distribution with a scale of 0.2 situates most of the density over smaller effect size values (i.e. between -2 and 2).

Overall, cauchy distributions tend to situate more density over largr effect sizes (versus normal distributions). This means that they are more conservative, as Bayes factors are slightly more like to favour the null model hypothessis (because large effect sizes are slightly less likely). It is for this reason why cauchy distributions are favoured in Bayesian analyses. 

This visualises that no prior may be weakly informative, however if we know that our effect might be small, a Cauchy distribution with a small scale might be our best estimate.

To produce these prior distributions,  I update my models whilst setting *prior_PD* as TRUE. This saves samples from the the prior distribution specified in the model. These samples can then be saved as dataframe for plotting.

## Investigating posterior distributions and ROPE intervals for each contrast

```{r}
"extracting posterior draws"
posteriors.default <- insight::get_parameters(fit.gamma.default.prior) %>%
  dplyr::mutate(prior = "default")

posteriors.cauchy.wide <- insight::get_parameters(fit.gamma.wide.cauchy) %>%
  dplyr::mutate(prior = "cauchy_wide")
  
posteriors.cauchy.narrow <- insight::get_parameters(fit.gamma.narrow.cauchy) %>%
  dplyr::mutate(prior = "cauchy_narrow")
  
posteriors <- rbind(posteriors.default, posteriors.cauchy.wide, posteriors.cauchy.narrow)

"posterior distributions overall"
ggplot(data = posteriors, aes(x = heading, fill = prior)) +
  geom_density(alpha = 0.3) +
  geom_vline(aes(xintercept = 0), size = 2) +
  geom_vline(aes(xintercept = -0.04), size = 2) +
  geom_vline(aes(xintercept = -0.14)) +
  geom_vline(aes(xintercept = -0.10)) +
  geom_vline(data = posteriors %>%
               dplyr::group_by(prior) %>%
               dplyr::summarise(MAP = map_estimate(heading)), aes(xintercept = MAP, colour = prior)) +
  scale_fill_manual(name = "Prior", labels = c("Cauchy, scale = 0.2", "Cauchy, scale = 1", "Normal, scale = 2.5"), values = c("yellow", "red", "blue")) +
  scale_colour_manual(name = "Maximum A Posteriori", labels = c("Cauchy, scale = 0.2", "Cauchy, scale = 1", "Normal, scale = 2.5"), values = c("yellow", "red", "blue")) + 
  xlim(-.20, .20) +
  theme_plot +
  theme(legend.position = c(0.7, 0.5))

"hdis for each prior"
hdi.default <- hdi(posteriors.default, ci = .89)
hdi.cauchy.narrow <- hdi(posteriors.cauchy.narrow, ci = .89)
hdi.cauchy.wide <- hdi(posteriors.cauchy.wide, ci = .89)

hdi.default
hdi.cauchy.narrow
hdi.cauchy.wide

"establishing the percentage of the HDIs within the ROPE interval"
percentage_in_rope_glm <- rope(fit.gamma, ci = 1, range = c(-0.04, 0))

percentage_in_rope_glm

"median from my HDIs of standardised effect size for each contrast"
median(posteriors$heading)
```

The following is generated from the example given here (https://easystats.github.io/bayestestR/articles/example1.html), here (https://easystats.github.io/bayestestR/articles/credible_interval.html) and here (https://easystats.github.io/bayestestR/articles/bayestestR.html).

Here we visualise the posterior distribution from our models alongside the prior distribution. The posteriors from each prior are very similar. The vertical line represents the Maximum A Posteriori (peak of the posterior) for each of the posterior distributions. These are also very similar, and represent the most probable value of the coefficient for the heading variable. 

Computing the 89% HDIs reveals that these are also the same for of the posterior distributions regardless of the prior. 89% is arbitrary but was proposed by McElreath (2014, 2018) as being the most stable value. This is then plotted on the posterior distribution. The posterior alongside the HDI provide a nice interpretation for the overall effect. Rather than proposing a point estimate as you might in frequentist statistics (i.e. the correlation is r = 0.12) under this Bayesian framework, we can propose "the most probable value for our effect based upon our data is -0.11, however effects ranging from -0.14 to -0.10 are also compatible with the data".

The Bayesian method does not rely on statistical significance in order to understand whether an effect is interesting - instead, the posterior distribution generates a probabilistic view of certain parameters and their uncertainty i.e. the coefficient for heading. Rather than conclude that there is simply a difference from a point null (0), one method used in Bayesian analyses is to construct a ROPE. 

ROPE stands for "region of practical equivalence". Comparing your alternative hypothesis to a single point null does not make much sense theoretically, as it is very likely there will be a difference (even if it is extremely small). I can define a ROPE which is an interval around a point null that is "practically equivalent" to it.

Kruschke (2018) proposes that for linear models, ROPE intervals are defined as 0.1 (half of a small effect size) multiplied by the standard deviation of the outcome variable. Ordinarily this would generate a ROPE interval of [-0.02, 0.02]. However this refers to the a 2 tailed hypothesis. I can be more specific. I know the direction of my effect (negative) thus I double the negative part of the interval and implement a one tailed ROPE interval [-0.04, 0]. These are defined as the **black** lines.

Hence when comparing the ROPE to the 89% HDIs, it provides a very intuitive interpretation of the probability of the effects:

"*89% of the the most credible values for standardised coefficient effect of my contrasts are not practically equivalent to the null*". 

## Probability of Direction

```{r}
p_direction(posteriors.default)
p_direction(posteriors.cauchy.narrow)
p_direction(posteriors.cauchy.wide)
```

The *Probability of Direction* is a value that ranges from 50% to 100%. It is interpreted as the probability that a parameter is positive or negative. Mathematically it is defined as the percentage of the posterior distribution that is the same sign as the median. The *Probability of Direction* is independent of the model and computed from posterior draws. 

It does not detail the magnitude or importance of an effect. You could have a PD of 100% concentrated within and 0.0001 - 0.0001 range. This would indicate that the effect was positive with high certainty, however the size of the effect would negligible. Consequently, this states only that the probability that an effect is positive significant and negative. In this respect, it is strongly related to the frequentist p value. 

For more information see:

(https://easystats.github.io/bayestestR/articles/probability_of_direction.html#methods-comparison)
(https://easystats.github.io/bayestestR/reference/p_direction.html#arguments)

## Posterior predictive distributions

```{r}
"observed responses"
y <- magnitudedata$FirstSteeringTime

"sample draws from the posterior distribution"
y.draws <- posterior_predict(fit.gamma.narrow.cauchy, draws = 50)

color_scheme_set("brightblue")
ppc_dens_overlay(y, y.draws[1:50, ])
```

*Posterior distributions* explain unknown parameters (standardised effect size) and provides the 95% most credible values for that parameter. *Posterior predictive distributions* refer to a distribution of future predicted data based upon the data already seen. 

If the model is a good fit, you should be able to simulate similar data based on data generated during the experiment.

My model demonstrates a reasonably good fit and predicts the peak location well.  

## Computing a Bayes factor

```{r message = FALSE, warning = FALSE}
"computing Bayes factor against point null"
bf.null <- bayesfactor_parameters(fit.gamma, null = 0)
bf.null

"computing Bayes factor against ROPE interval"
bf.rope.narrow.cauchy <- bayesfactor_parameters(fit.gamma.narrow.cauchy, null = c(-0.04, 0))
bf.rope.narrow.cauchy

bf.rope.wide.cauchy <- bayesfactor_parameters(fit.gamma.wide.cauchy, null = c(-0.04, 0))
bf.rope.wide.cauchy

bf.rope.default <- bayesfactor_parameters(fit.gamma.default.prior, null = c(-0.04, 0))
bf.rope.default
```

This is based upon the example given here (https://easystats.github.io/bayestestR/articles/bayes_factors.html#testing-models-parameters-with-bayes-factors)

I can compute my Bayes factor for my heading coefficient for models with different priors. Firstly against the point null. This provides extremely large Bayes factors as evidence against a point null is very likely.

After this, I compute a Bayes factor against the ROPE value I used to investigate my posterior distribution. The Bayes factor is still very large, suggesting my effect is substantial. Bayes factors are larger for the model with a prior specified as a narrow cauchy distribution, followed by a wide cauchy distribution, and then a normal distribution. This is because the cauchy priors are more conservative. Hence the shift of evidence if greater from prior to posterior resulting in a higher Bayes factor. 

## Support intervals

```{r}
"computing support intervals for models with each prior"
my_si.default <- si(fit.gamma.default.prior, BF = 10)
my_si.cauchy.narrow <- si(fit.gamma.narrow.cauchy, BF = 10)
my_si.cauchy.wide <- si(fit.gamma.wide.cauchy, BF = 10)

my_si.default
my_si.cauchy.narrow
my_si.cauchy.wide

ggplot(data = posteriors, aes(x = heading, fill = prior)) +
  geom_density(alpha = 0.3) +
  geom_vline(aes(xintercept = -0.15)) +
  geom_vline(aes(xintercept = -0.09)) +
  geom_vline(data = posteriors %>%
               dplyr::group_by(prior) %>%
               dplyr::summarise(MAP = map_estimate(heading)), aes(xintercept = MAP, colour = prior)) +
  scale_fill_manual(name = "Prior", labels = c("Cauchy, scale = 0.2", "Cauchy, scale = 1", "Normal, scale = 2.5"), values = c("yellow", "red", "blue")) +
  scale_colour_manual(name = "Maximum A Posteriori", labels = c("Cauchy, scale = 0.2", "Cauchy, scale = 1", "Normal, scale = 2.5"), values = c("yellow", "red", "blue")) + 
  xlim(-.20, .20) +
  theme_plot +
  theme(legend.position = c(0.5, 0.5))
```

Uninformative prior distributions assign uniform credibility to values of my effect size (i.e. coefficients). As data comes in, values of my effect size receive more or less credibility. This is represented via the posterior distribution. The reallocation of this credibility is represented by the updating (Bayes) factor. Hence the Bayes factor represents the increase/decrease in credibility of values of theta based upon evidence (data).

The support interval is a defined range of theta values that receive a certain level of support from the data. Regardless of prior, the support intervals are the same and suggest that a majority of values within the posterior distributions would generate a Bayes factor of at least 10 for evidence of an affect that heading has on reaction time. 

They are analogous to frequentist confidence intervals. 95% confidence intervals encompass the true population parameter 95% of the time. Support intervals encompass values of the coefficient that receive a certain amount of evidence given the data (in this example, the support intervals highlight the coefficient values that are associated with substantial support from the data).

**Why would I use them?**

Within Bayesian statistics, there exists a conceptual difference between the Bayes factor and the 95% credible interval. 

- The Bayes factor aims to quantify the presence or absence of an effect 
- The credible interval quantifies the size of the effect under the assumption that it is present

Because of this there can be a paradoxical situation whereby the Bayes factor supports a point null hypothesis model versus the alternative but simultaneously the credible interval excludes a theta value of 0 i.e. no effect.

In an attempt to solve this problem, support intervals quantify which values of theta (i.e. coefficient) are supported by the data. This is generated by computing a range of values that predicted the data well. How well depends on what you specify the level of evidence you want (i.e. via the Bayes factor). 

## Model comparison with loo

```{r message = FALSE, warning = FALSE}
fit.inverse.gaussian.narrow.cauchy <- stan_glmer(FirstSteeringTime ~ heading + (1 + heading | pNum),
                                                 family = inverse.gaussian(link = "identity"),
                                                 prior = cauchy(location = 0, scale = 0.2),
                                                 data = magnitudedata)

fit.inverse.gaussian.wide.cauchy <- stan_glmer(FirstSteeringTime ~ heading + (1 + heading | pNum),
                                               family = Gamma(link = "identity"),
                                               prior = cauchy(location = 0, scale = 1),
                                               data = magnitudedata)

loo.inverse.gaussian <- loo(fit.inverse.gaussian.narrow.cauchy)

loo.gamma <- loo(fit.gamma.narrow.cauchy)

print(loo_compare(loo.inverse.gaussian, loo.gamma), digits = 3)
```
The following example is adapted from (https://mc-stan.org/loo/reference/loo_compare.html)

The *loo_compare* function compares the two Bayesian model fits (one with inverse gaussian distribution, the other with a gamma distribution) using the "leave one out" cross validation method. This method investigates the differences in predictive errors and standard errors for each model. This is encapsulated by the *elpd_diff* (difference in predictive errors) and *se_diff* (difference in standard errors) variables.

Columns within the matrix are computed by generating pairwise comparisons between each model and the model with highest ELPD (i.e. highest predictive accuracy). Because of this, the first row will always be zeros because this is the preferred model compared against itself. 

When the difference (*elpd_diff*) is positive, the expected predictive accuracy is higher for the second model. When it is negative, this favours the model in the first row. Because my *elpd_diff* shows a large negative for the fit.gamma.narrow.cauchy model, the fit.inverse.gaussian.narrow.cauchy is the preferred model and the one we expand upon below. 

## Assesing posterior of best fitting model

```{r}
"posterior draws"
posteriors.inverse.gaussian <- insight::get_parameters(fit.inverse.gaussian.narrow.cauchy)

"89% HDIs"
hdi.inverse.gausian <- hdi(posteriors.inverse.gaussian, ci = .89)
hdi.inverse.gausian

"plotting posteriors"
ggplot(data = posteriors.inverse.gaussian, aes(x = heading)) +
  geom_density(alpha = 0.3, fill = "blue") +
  geom_vline(aes(xintercept = 0), size = 2) +
  geom_vline(aes(xintercept = -0.04), size = 2) +
  geom_vline(aes(xintercept = -0.13)) +
  geom_vline(aes(xintercept = -0.10)) +
  geom_vline(aes(xintercept = map_estimate(posteriors.inverse.gaussian$heading), colour = "red")) +
  geom_vline(aes(xintercept = mean(posteriors.inverse.gaussian$heading), colour = "yellow")) +
  geom_vline(aes(xintercept = median(posteriors.inverse.gaussian$heading), colour = "orange")) +
  scale_colour_manual(name = "Statistic", labels = c("Maximum A Posteriori", "Mean", "Median"), values = c("red", "yellow", "orange")) + 
  xlim(-.20, .20) +
  theme_plot +
  theme(legend.position = c(0.7, 0.5))

"computing Bayes factor against point null"
bf.null.inverse.gaussian <- bayesfactor_parameters(fit.inverse.gaussian.narrow.cauchy, null = 0)
bf.null.inverse.gaussian

"computing Bayes factor against ROPE"
bf.rope.inverse.gaussian <- bayesfactor_parameters(fit.inverse.gaussian.narrow.cauchy, null = c(-0.04, 0))
bf.rope.inverse.gaussian

```









