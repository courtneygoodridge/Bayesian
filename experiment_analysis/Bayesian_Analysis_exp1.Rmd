---
title: "Bayesian analysis - draft write up"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This markdown document is my first attempt at writing up a Bayesian analysis of my data. The data I am using is from experiment 1. For this example, I will be investigating how heading angle affects RTs. For a reminder, both Threshold and Accumulator models propose reduced RTs as heading increases.

I am using the *rstanarm* package to fit a Bayesian generalised linear models to my data. I then compute Bayes factor for the full model contrasts, Bayes factors for the restricted model, invetsigate 95% HDIs, use the HDI + ROPE for inference and the probability of direction.

## Load packages 

```{r message = FALSE}
library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(ggplot2) # data vis
library(BayesFactor) # performs Bayesian ANOVA
library(rstanarm) # bayesian regression modelling
library(bayestestR) # describes Bayesian models and posterior distributions
library(bayesplot) # allows plots for posterior predictive distributions
library(loo) # Bayesian model comparison
library(arm) # computes Bayes factor for glm model
library(insight) # get posterior parameters 
library(fitdistrplus) # investigating the distribution of my data
library(emmeans) # for computing estimated marginal means for Bayesian models
library(fitdistrplus) # finding shape distribution
```

## Load data

```{r}
# rm(list = ls()) - clear global environment
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/experiment_1/Data cleaning and modelling/Finished_code")
magnitudedata <- read.csv("magnitudedata.csv")

magnitudedata <- magnitudedata %>%
  dplyr::filter(EarlyResponses == FALSE)
```


## Fitting Bayesian regression model

```{r message = FALSE, warning = FALSE}
fit.gamma <- stan_glmer(FirstSteeringTime ~ heading + (1 + heading | pNum),
                        family = Gamma(link = "identity"),
                        data = magnitudedata)
summary(fit.gamma)

fit.gamma.glm <- stan_glm(FirstSteeringTime ~ heading,
                        family = Gamma(link = "identity"),
                        data = magnitudedata)

summary(fit.gamma.glm)
```

The stan_glmer function allows me to fit a Bayesian multilevel. This is necessary for my data structure as if I fit a standard glm, it assumes that every observation is actually a participant. This will inflate my estimate.

The function also requires a prior. I omitted the option of a prior and defaulted to the weakly informative ones. This is because as the documentation indicates that for many applications, they will work well (https://www.rdocumentation.org/packages/rstanarm/versions/2.19.2/topics/priors)

I set adapt_delta arguement to *0.999* to reduce the stepsize during the model simulation. Stepsize specifies the resolution at which you are simulating through the posterior distribution. If the steps are too large, I might miss something and thus estimates become biased. This is known as divergence. The documentation proposes an adapt_delta arguement value such as this to reduce the chances of divergence (http://mc-stan.org/rstanarm/articles/aov.html)

Rhat values suggest the model succesfully converged.

*mean_ppd* in the estimates table can be used as a heuristic for how well the model describes the data. This value represents a best guess of the *mean posterior predicitive distribution* (distribution of simiulated data based on current data). Ideally this should be equal to the mean of my outcome variable (RT). In this case, the values are similar.

### Investigating prior distributions

```{r}
prior_summary(fit.gamma)

fit.gamma.prior <- update(fit.gamma, prior_PD = TRUE)

priors <- as.data.frame(fit.gamma.prior)

ggplot(data = priors) +
  geom_density(aes(x = heading), fill = "red", alpha = 0.1) +
  ggtitle("Prior distribution for heading")
```

The prior summary function gives me a summary of my prior. As I did not specify one, it provides a weakly informative one consisting of a normal distribution centred at 0.

By updating my model and setting *prior_PD* as TRUE, it saves samples from the prior distribution. Thus *fit.gamma.prior* and *fit.gamma* are the save objects put the first samples from the priors and the latter from the posterior distribution.

I then save the new model as a dataframe. This gives me access to the prior distributions for the coefficients. I then plot these to highlight the vaguely informative prior distribution. 

One comment that I could make is that I am using a prior that is too wide. The distribution I am using is normal but the scale is 10. It is unlikely that my effect will be this large and the setting of priors influences the Bayes factor. Thus I might want a prior that situates less density over large effect sizes, as these generally less likely in these experiments. 

## Investigating posterior distributions and ROPE intervals for each contrast

```{r}
posteriors <- insight::get_parameters(fit_glm)

ggplot(data = posteriors) +
  geom_vline(xintercept = median(posteriors$heading), color = "red") +
  geom_density(aes(x = heading), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = -.04, color = "black", size = 2) +
  geom_vline(xintercept = 0, color = "black", size = 2) +
  xlim(-.15, .15) +
  xlab("standardised coefficient effect")

# establishing the percentage of the HDIs within the ROPE interval
percentage_in_rope_glm <- rope(fit_glm, ci = 1, range = c(-0.04, 0))

percentage_in_rope_glm

# median from my HDIs of standardised effect size for each contrast

median(posteriors$heading)
```
The following is generated from the example given here (https://easystats.github.io/bayestestR/articles/example1.html)

The Bayesian GLM demonstrates that for each contrast, heading level is having an negative on the outcome variable i.e. an increase in heading is generating a decrease in RT. However, the Bayesian method does not rely on statistical sigificance in order to understand whether an effect is interesting - instead, the posterior distribution generates a probabilistic view of certain parameters and their uncertainty i.e. the coefficient for certain contrasts. Rather than conclude that there is simply a difference from a point null (0) one method used in Bayesian analyses is to construct a ROPE. 

ROPE stands for "region of practical equivalence". Comparing your alternative hypothesis to a single point null does not make much sense theoretically, as it is very likely there will be a difference (even if it is extremely small). I can define a ROPE which is an interval around a point null that is "practically equivalent" to it.

Kruschke (2018) proposes that for linear models, ROPE intervals are defined as 0.1 (half of a small effect size) multiplied by the standard deviation of the outcome variable. Ordinarily this would generate a ROPE interval of [-0.02, 0.02]. 

However this refers to the a 2 tailed hypothesis. I can be more specific. I know the direction of my effect (negative) thus I double the negative part of the interval and implement a one tailed ROPE interval [-0.04, 0]. These are defined as the **black** lines.

I can then draw from the posterior distributions, plot the HDIs for each of my contrasts and indicate the median most credible value with a vertical line. This provides a nice literal interpretation 

"*95% of the the most credible values for standardised coefficient effect of my contrasts are not practically equivalent to the null*". 

**Sidenote** I get a warning of multicollinearity between 2 of my contrasts (heading1 and heading2). This likely due to my contrast matrix being linearly dependent. 

## Probability of Direction

```{r}

p_direction(posteriors)

```

The *Probability of Direction* is a value that ranges from 50% to 100%. It is interpretted as the probability that a parameter is positive or negative. Mathematically it is defined as the percentage of the posterior distribition that is the same sign as the median. The *Probability of Direction* is indepedent of the model and computed from posterior draws. 

It does not detail the magnitude or importance of an effect - rather the probability that an effect is positive and negative. In this respect, it is directly relatable to the frequentist p value. 

For more information see:

(https://easystats.github.io/bayestestR/articles/probability_of_direction.html#methods-comparison)
(https://easystats.github.io/bayestestR/reference/p_direction.html#arguments)

## Posterior predictive distributions

```{r}
# observedresponses
y <- magnitudedata$FirstSteeringTime

# sample draws from the posterior distribution
yrep_glm <- posterior_predict(fit_glm, draws = 50)

color_scheme_set("brightblue")
ppc_dens_overlay(y, yrep_glm[1:50, ])
```

*Posterior distributions* explain unknown parameters (standardised effect size) and provide the 95% most credible values for that parameter. *Posterior predictive distributions* refer to a distribution of future predicted data based upon the data already seen. 

If the model is a good fit, you should be able to simulate similar data based on data generated during the experiment.

My model demonstrates a reasonably good fit and predicts the peak location well, however it under predicts the density of the peak. 

## Computing a Bayes factor

```{r message = FALSE, warning = FALSE}

bf_glm <- bayesfactor_parameters(fit_glm, null = 0)
bf_glm

```

This is based upon the example given here (https://easystats.github.io/bayestestR/articles/bayes_factors.html#testing-models-parameters-with-bayes-factors)

I can compute my Bayes factor for each contrast. This also allows me to compute my Bayes factor for the alternative hypothesis versus the ROPE interval null hypothesis, rather than a point null. This demonstrates substantial evidence for the alternative hypotheses for each contrast.

## Support intervals

```{r}
my_si <- si(fit_glm, BF = 30)
my_si

ggplot(data = posteriors) +
  geom_vline(xintercept = median(posteriors$heading), color = "red") +
  geom_density(aes(x = heading), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = -.04, color = "black", size = 2) +
  geom_vline(xintercept = 0, color = "black", size = 2) +
  geom_vline(xintercept = -.12, color = "gray", size = 2) +
  geom_vline(xintercept = -.10, color = "gray", size = 2) +
  xlim(-.15, 0) +
  xlab("standardised coefficient effect")
```

Uninformative prior distributions assign uniform credibility to values of theta (i.e. coefficients). As data comes in, values of theta recieve more or less credibility. This is represented via the posterior distribution. The reallocation of this credibility is represented by the updating (Bayes) factor. Hence the Byaes factor represents the increase/decrease in credibility of values of theta based upon evidence (data).

The support interval is a defined range of theta values that  recieve a certain level of support from the data. 

They are analogous to frequentist confidence intervals. 95% confidence intervals encompass teh true population parameter 95% of the time. Support intervals encompass values of the coefficient that recieve a certain amount of evidence given the data (in this example, the support intervals highlight the coefficient values that are associated with substantial support frokm the data).

**Why would I use them?**

Within Bayesian statistics, there exists a conceptual difference between the Bayes factor and the 95% credible interval. 

- The Bayes factor aqims to quantify the presence or absence of an effect 
- The credible interval quantifies the size of the effect under the assumption that it is present

Because of this there can be a paradoxical situation whereby the Bayes factor supports a point null hypothesis model versus the alternative but simultaneously the credible interval excludes a theta value of 0 i.e. no effect.

In an attempt to solve this problem, support intervals quantify which values of theta (i.e. coefficient) are supported by the data. This is generated by computing a range of values that predicted the data well. How well depends on what you specifiy the level of evidence you want (i.e. via the Bayes factor). 

## Restriction Bayesian model

```{r}
contrasts(magnitudedata$heading) <- "contr.bayes"

fit_bayes <- stan_glm(FirstSteeringTime ~ heading,
                      family = Gamma(link ="identity"),
                      adapt_delta = 0.999,
                      data = magnitudedata)

em_bayes <- emmeans(fit_bayes, ~ heading)
fit_bayes_prior <- update(fit_bayes, prior_PD = TRUE)
em_bayes_prior <- emmeans(fit_bayes_prior, ~ heading)

hyp <- c("`2` < `1.5` & `1.5` < `1` & `1` < `0.5`")

bayesfactor_restricted(posterior = em_bayes,
                       prior = em_bayes_prior,
                       hypothesis = hyp)
```

Thus far I have computed Bayes factors for each of my contrasts. However I can be more specific as I know the direct of my effect (increased heading leads to a decrease in RTs). Thus I can implement a restriction model and calculate a Bayes factor versus an unrestricted model i.e. just differences between my heading levels. The details of this code chunk can be found in the *Bayesian_restriction.Rmd* markdown file.

## Bayesian GLM model with lognormal family and model comparison

```{r message = FALSE, warning = FALSE}
fit_glm_log <- stan_glm(FirstSteeringTime ~ heading, 
                        family = gaussian(link = "log"), 
                        adapt_delta = 0.999, 
                        data = magnitudedata)

loo_glm <- loo(fit_glm)

loo_log <- loo(fit_glm_log)

print(loo_compare(loo_log, loo_glm), digits = 3)
```
The *loo_compare* function compares the two Bayesian model fits using the "leave one out" cross validaion method. This method investigates the differences in predictive errors and standard errors for each model. This is encapsulated by the *elpd_diff* (difference in predictive errors) and *se_diff* (difference in standard errors) variables.

Columns within the matrix are computed by generating pairwise comparisons between each model and the model with highest ELPD (i.e. highest predictive accuracy). Because of this, the first row will always be zeros because this is the preferred model compared against itself. 

When the difference (*elpd_diff*) is positive, the expected predictive accuracy is higher for the second model. When it is negative, this favours the model in the first row. Because my *elpd_diff* shows a large negative, model2 (fit_glm) is the preferred model. 

