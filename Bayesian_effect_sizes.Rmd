---
title: "Estimating effect sizes for Bayesian models"
output: html_document
---

This R markdown will specify how to compute the effect sizes of Bayesian models. The first part will utilise the iris dataframe from the online example. I will then investigate how to standardise my coefficients in order to use effect size guidelines for my contrasts. I will utilise the rstanarm package. I will be following examples from the following online resources:

Effect sizes for Bayesian models:
(https://easystats.github.io/effectsize/articles/bayesian_models.html)

Parameter standardisation function:
(https://easystats.github.io/effectsize/reference/standardize_parameters.html#details)
(https://easystats.github.io/effectsize/articles/standardize_parameters.html#standardized-differences)
(https://easystats.github.io/effectsize/articles/interpret.html)


Obtaining indices of effect for Bayesian models is currently an issue, but it can be done to an extent via one of two methods:

1) To compute standardised parameters.
2) To compute effect sizes via test statistic approximations.

I will use the iris data set to compute examples for test statistic approximation, and and my own data set for computing standardised parameters.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

```{r}
# rm(list = ls())
library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(ggplot2) # data vis
library(BayesFactor) # performs Bayesian ANOVA
library(rstanarm) # bayesian regression modelling
library(bayestestR) # describes Bayesian models and posterior distributions
library(bayesplot) # allows plots for posterior predictive distributions
library(loo) # Bayesian model comparison
library(arm) # computes Bayes factor for glm model
library(insight) # get posterior parameters 
library(fitdistrplus) # checking distribution
library(effectsize) # for computing Bayesian model effect sizes 
library(parameters)
```

## Load iris data

```{r}
data("iris")
```

## Frequentist mulitple regression example

```{r}
df <- iris[, 1:4]  # Remove the Species factor

model <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = df) 

parameters <- model_parameters(model)[2:4,]
convert_t_to_r(parameters$t, parameters$df_residual)
```
First I remove species factor from the iris dataframe.

For frequentist methods, the aim is to generate a coefficient that matches the partial correlations. Partial correlations are between 2 continuous variables whilst controlling for other variables. Thus we are able to see the direct effect of a variable upon another. In order to do this, I extract the parameters from the multiple regression model and convert the t statistics to a correlation.

Partial correlations range from -1 (perfect negative linear relationship with continuous variable) or +1 (perfect positive linear relationship with continuous variable). 

## Bayesian multiple regression example

```{r}
model_bayes <- stan_glm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = df)

parameters <- model_parameters(model)[2:4,]
convert_t_to_r(parameters$t, parameters$df_residual)
```

The Bayesian alternative estimates the posterior partial correlations by approximating the t statistic for each variable. This is done by dividing the coefficient by the standard deviation of the posterior. This is a hybrid method how it seems to match the linear multiple regression output.

## Conclusions

Test statistic approixmation works well for multiple regression models, however I am applying contrasts between levels of my indepedent variables. Thus being able to standardise these coefficients would generate effect size estimates for each of my contrasts. 

## Load in experiment 1 data

```{r}
# setwd("C:/Users/Courtney/Documents/PhD/Project/Experiment_code/experiment_1")
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/experiment_1")
temp = list.files(pattern = c("magnitudedata", "*.csv")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
magnitudedata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe
```

## Fitting Bayesian model

```{r}
magnitudedata$heading <- as.factor(magnitudedata$heading)

# default contrasts - orthogonal
contrasts(magnitudedata$heading)

contrastmatrix <- cbind(c(-1, 1, 0, 0), c(0, -1, 1, 0), c(0, 0, -1, 1))

contrasts(magnitudedata$heading) <- contrastmatrix

contrasts(magnitudedata$heading)

my_model_bayes <- stan_glm(FirstSteeringTime ~ heading,
                           family = gaussian("log"),
                           adapt_delta = 0.999,
                           data = magnitudedata)
```

For the purpose of this example I have left the default contrasts as they are. This means that each heading is contrasted against zero, thus I'll compute the pure effects of just that heading without contrasting against others. I could implement a contrast matrix and compute effect sizes for different contrasts but I won't for now. 

## Standardising posteriors

```{r}
posthoc_model <- standardize_posteriors(my_model_bayes,
                                                  method = "posthoc",
                                                  robust = TRUE)

smart_model <- standardize_posteriors(my_model_bayes,
                                                  method = "smart",
                                                  robust = TRUE)

basic_model <- standardize_posteriors(my_model_bayes,
                                                  method = "basic",
                                                  robust = TRUE)
```

The *standardize_posteriors* function standardises the coefficients of the model that is inputted. There are many methods that can be used to compute the standardisation. Specifics can be found here:
(https://easystats.github.io/effectsize/reference/standardize_parameters.html#details)

*posthoc*
If I keep heading as a numeric data type, *posthoc* might be the best the option. Coefficients are divided by the SD of the outcome variables to generate "expression units". Coefficients that relate to numeric variables are then multiplied by the SD of the related terms. The result of this is that 1 SD increase in the IV relates to a a specific change in the outcome variable. 

The only problem with this is that the interpretation does not work for factors or binary variables and thus this method used on these variable types means that coefficients relate to a change in level. This is not the end of the world but I would need to convert my heding values into factors in order to implement the restriction model for calculating the Bayes factor. 

*smart*
The *smart* method is similar to the *posthoc* method in that it does not require a full refit of the model. The main difference is that the SD of the response is computed on the relevant section of data. For example, if I had a factor with 3 levels (intercept, B and C) the scaling is done by the variance in the intercept only. This is similar to Glass' Delta and where only the the intercept is used. This should be used if the SDs are significantly different for responses (https://www.statisticshowto.datasciencecentral.com/glasss-delta/).

*basic*
The *basic* method treat all variables as continuous variables. 

In the example, they use all of these methods with factors inputted into the model so perhaps they are all okay to use. 

